#h) lowest dev corresponding size
cv.OJ
#i) prune
prune.OJ=prune.misclass(tree.OJ,best=5)
plot(prune.OJ)
text(prune.OJ,pretty=0)
#j) new cond matrix in train
pred.train.5=predict(prune.OJ,OJ.train,type="class")
cm.train.5 <- table(pred.train.5,OJ.train$Purchase)
cm.train.5
#Accuracy in train
(cm.train.5[1] + cm.train.5[4])/nrow(OJ.train)
#k) new cond matrix in test
pred.test.5=predict(prune.OJ,OJ.test,type="class")
cm.test.5 <- table(pred.test.5,OJ.test$Purchase)
cm.test.5
#Accuracy in test
(cm.test.5[1] + cm.test.5[4])/nrow(OJ.test)
#Exercise 4
#a) splitting
set.seed(42)
train=sample(1:nrow(Carseats), (80/100)*nrow(Carseats))
Carseats.train=Carseats[train,]
Carseats.test=Carseats[-train,]
#b) fitting & ploting
tree.carseats=tree(Sales~.,Carseats.train)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats,pretty=0)
pred.train=predict(tree.carseats,Carseats.train)
mse.train <- mean((pred.train - Carseats.train$Sales)^2)
mse.train
pred.test=predict(tree.carseats,Carseats.test)
mse.test <- mean((pred.test - Carseats.test$Sales)^2)
mse.test
#c) cross validation
set.seed(42)
cv.Carseats <- cv.tree(tree.carseats)
cv.Carseats
plot(cv.Carseats$size,cv.Carseats$dev,type="b")
# best k is 21 which is original size, so there is no reason to prune
#prune.carseats=prune.tree(tree.carseats,best=<Best K>)
#d) bagging
#e) random forest
bag.Carseats=randomForest(Sales~.,data=Carseats.train,mtry=10,importance=TRUE)
bag.Carseats
yhat.bag = predict(bag.Carseats, newdata=Carseats.test)
plot(yhat.bag, Carseats.test$Sales)
abline(0,1)
mean((yhat.bag- Carseats.test$Sales)^2)
set.seed(42)
bag.Carseats=randomForest(Sales~.,data=Carseats.train,mtry=4,importance=TRUE)
bag.Carseats
yhat.bag = predict(bag.Carseats, newdata=Carseats.test)
plot(yhat.bag, Carseats.test$Sales)
abline(0,1)
mean((yhat.bag- Carseats.test$Sales)^2)
# install.packages("tree")
library(tree)
# install.packages("ISLR")
library(ISLR)
# install.packages("randomForest")
library(randomForest)
# Exercise 3
#?OJ
#a) training and test set
set.seed(42)
train=sample(1:nrow(OJ), 800)
OJ.train=OJ[train,]
OJ.test=OJ[-train,]
#b) fit tree
tree.OJ=tree(Purchase~.,OJ.train)
summary(tree.OJ)
#c) interpret one terminal node
tree.OJ
#d) plotting
plot(tree.OJ)
text(tree.OJ,pretty=0)
#e) test / train conf matrix
#test
pred.test=predict(tree.OJ,OJ.test,type="class")
cm.test <- table(pred.test,OJ.test$Purchase)
cm.test
#Accuracy in test
(cm.test[1] + cm.test[4])/nrow(OJ.test)
#train
pred.train=predict(tree.OJ,OJ.train,type="class")
cm.train <- table(pred.train,OJ.train$Purchase)
cm.train
#Accuracy in train
(cm.train[1] + cm.train[4])/nrow(OJ.train)
#f) cv.tree
set.seed(42)
cv.OJ=cv.tree(tree.OJ,FUN=prune.misclass)
#g) plotting
plot(cv.OJ$size,cv.OJ$dev,type="b")
#h) lowest dev corresponding size
cv.OJ
#i) prune
prune.OJ=prune.misclass(tree.OJ,best=5)
plot(prune.OJ)
text(prune.OJ,pretty=0)
#j) new cond matrix in train
pred.train.5=predict(prune.OJ,OJ.train,type="class")
cm.train.5 <- table(pred.train.5,OJ.train$Purchase)
cm.train.5
#Accuracy in train
(cm.train.5[1] + cm.train.5[4])/nrow(OJ.train)
#k) new cond matrix in test
pred.test.5=predict(prune.OJ,OJ.test,type="class")
cm.test.5 <- table(pred.test.5,OJ.test$Purchase)
cm.test.5
#Accuracy in test
(cm.test.5[1] + cm.test.5[4])/nrow(OJ.test)
#Exercise 4
#a) splitting
set.seed(42)
train=sample(1:nrow(Carseats), (70/100)*nrow(Carseats))
Carseats.train=Carseats[train,]
Carseats.test=Carseats[-train,]
#b) fitting & ploting
tree.carseats=tree(Sales~.,Carseats.train)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats,pretty=0)
pred.train=predict(tree.carseats,Carseats.train)
mse.train <- mean((pred.train - Carseats.train$Sales)^2)
mse.train
pred.test=predict(tree.carseats,Carseats.test)
mse.test <- mean((pred.test - Carseats.test$Sales)^2)
mse.test
#c) cross validation
set.seed(42)
cv.Carseats <- cv.tree(tree.carseats)
cv.Carseats
plot(cv.Carseats$size,cv.Carseats$dev,type="b")
# best k is 21 which is original size, so there is no reason to prune
#prune.carseats=prune.tree(tree.carseats,best=<Best K>)
#d) bagging
set.seed(42)
bag.Carseats=randomForest(Sales~.,data=Carseats.train,mtry=10,importance=TRUE)
bag.Carseats
yhat.bag = predict(bag.Carseats, newdata=Carseats.test)
plot(yhat.bag, Carseats.test$Sales)
abline(0,1)
mean((yhat.bag- Carseats.test$Sales)^2)
#e) random forest
set.seed(42)
bag.Carseats=randomForest(Sales~.,data=Carseats.train,mtry=4,importance=TRUE)
bag.Carseats
yhat.bag = predict(bag.Carseats, newdata=Carseats.test)
plot(yhat.bag, Carseats.test$Sales)
abline(0,1)
mean((yhat.bag- Carseats.test$Sales)^2)
# install.packages("tree")
library(tree)
# install.packages("ISLR")
library(ISLR)
# install.packages("randomForest")
library(randomForest)
# Exercise 3
#?OJ
#a) training and test set
set.seed(42)
train=sample(1:nrow(OJ), 800)
OJ.train=OJ[train,]
OJ.test=OJ[-train,]
#b) fit tree
tree.OJ=tree(Purchase~.,OJ.train)
summary(tree.OJ)
#c) interpret one terminal node
tree.OJ
#d) plotting
plot(tree.OJ)
text(tree.OJ,pretty=0)
#e) test / train conf matrix
#test
pred.test=predict(tree.OJ,OJ.test,type="class")
cm.test <- table(pred.test,OJ.test$Purchase)
cm.test
#Accuracy in test
(cm.test[1] + cm.test[4])/nrow(OJ.test)
#train
pred.train=predict(tree.OJ,OJ.train,type="class")
cm.train <- table(pred.train,OJ.train$Purchase)
cm.train
#Accuracy in train
(cm.train[1] + cm.train[4])/nrow(OJ.train)
#f) cv.tree
set.seed(42)
cv.OJ=cv.tree(tree.OJ,FUN=prune.misclass)
#g) plotting
plot(cv.OJ$size,cv.OJ$dev,type="b")
#h) lowest dev corresponding size
cv.OJ
#i) prune
prune.OJ=prune.misclass(tree.OJ,best=5)
plot(prune.OJ)
text(prune.OJ,pretty=0)
#j) new cond matrix in train
pred.train.5=predict(prune.OJ,OJ.train,type="class")
cm.train.5 <- table(pred.train.5,OJ.train$Purchase)
cm.train.5
#Accuracy in train
(cm.train.5[1] + cm.train.5[4])/nrow(OJ.train)
#k) new cond matrix in test
pred.test.5=predict(prune.OJ,OJ.test,type="class")
cm.test.5 <- table(pred.test.5,OJ.test$Purchase)
cm.test.5
#Accuracy in test
(cm.test.5[1] + cm.test.5[4])/nrow(OJ.test)
#Exercise 4
#a) splitting
set.seed(42)
train=sample(1:nrow(Carseats), (70/100)*nrow(Carseats))
Carseats.train=Carseats[train,]
Carseats.test=Carseats[-train,]
#b) fitting & ploting
tree.carseats=tree(Sales~.,Carseats.train)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats,pretty=0)
pred.train=predict(tree.carseats,Carseats.train)
mse.train <- mean((pred.train - Carseats.train$Sales)^2)
mse.train
pred.test=predict(tree.carseats,Carseats.test)
mse.test <- mean((pred.test - Carseats.test$Sales)^2)
mse.test
#c) cross validation
set.seed(42)
cv.Carseats <- cv.tree(tree.carseats)
cv.Carseats
plot(cv.Carseats$size,cv.Carseats$dev,type="b")
# best k is 21 which is original size, so there is no reason to prune
#prune.carseats=prune.tree(tree.carseats,best=<Best K>)
#d) bagging
set.seed(42)
bag.Carseats=randomForest(Sales~.,data=Carseats.train,mtry=10,importance=TRUE)
bag.Carseats
yhat.bag = predict(bag.Carseats, newdata=Carseats.test)
plot(yhat.bag, Carseats.test$Sales)
abline(0,1)
mean((yhat.bag- Carseats.test$Sales)^2)
#e) random forest
set.seed(42)
bag.Carseats=randomForest(Sales~.,data=Carseats.train,mtry=4,importance=TRUE)
bag.Carseats
yhat.bag = predict(bag.Carseats, newdata=Carseats.test)
plot(yhat.bag, Carseats.test$Sales)
abline(0,1)
mean((yhat.bag- Carseats.test$Sales)^2)
# install.packages("tree")
library(tree)
# install.packages("ISLR")
library(ISLR)
# install.packages("randomForest")
library(randomForest)
# Exercise 3
#?OJ
#a) training and test set
set.seed(42)
train=sample(1:nrow(OJ), 800)
OJ.train=OJ[train,]
OJ.test=OJ[-train,]
#b) fit tree
tree.OJ=tree(Purchase~.,OJ.train)
summary(tree.OJ)
#c) interpret one terminal node
tree.OJ
#d) plotting
plot(tree.OJ)
text(tree.OJ,pretty=0)
#e) test / train conf matrix
#test
pred.test=predict(tree.OJ,OJ.test,type="class")
cm.test <- table(pred.test,OJ.test$Purchase)
cm.test
#Accuracy in test
(cm.test[1] + cm.test[4])/nrow(OJ.test)
#train
pred.train=predict(tree.OJ,OJ.train,type="class")
cm.train <- table(pred.train,OJ.train$Purchase)
cm.train
#Accuracy in train
(cm.train[1] + cm.train[4])/nrow(OJ.train)
#f) cv.tree
set.seed(42)
cv.OJ=cv.tree(tree.OJ,FUN=prune.misclass)
#g) plotting
plot(cv.OJ$size,cv.OJ$dev,type="b")
#h) lowest dev corresponding size
cv.OJ
#i) prune
prune.OJ=prune.misclass(tree.OJ,best=5)
plot(prune.OJ)
text(prune.OJ,pretty=0)
#j) new cond matrix in train
pred.train.5=predict(prune.OJ,OJ.train,type="class")
cm.train.5 <- table(pred.train.5,OJ.train$Purchase)
cm.train.5
#Accuracy in train
(cm.train.5[1] + cm.train.5[4])/nrow(OJ.train)
#k) new cond matrix in test
pred.test.5=predict(prune.OJ,OJ.test,type="class")
cm.test.5 <- table(pred.test.5,OJ.test$Purchase)
cm.test.5
#Accuracy in test
(cm.test.5[1] + cm.test.5[4])/nrow(OJ.test)
#Exercise 4
#a) splitting
set.seed(42)
train=sample(1:nrow(Carseats), (80/100)*nrow(Carseats))
Carseats.train=Carseats[train,]
Carseats.test=Carseats[-train,]
#b) fitting & ploting
tree.carseats=tree(Sales~.,Carseats.train)
summary(tree.carseats)
plot(tree.carseats)
text(tree.carseats,pretty=0)
pred.train=predict(tree.carseats,Carseats.train)
mse.train <- mean((pred.train - Carseats.train$Sales)^2)
mse.train
pred.test=predict(tree.carseats,Carseats.test)
mse.test <- mean((pred.test - Carseats.test$Sales)^2)
mse.test
#c) cross validation
set.seed(42)
cv.Carseats <- cv.tree(tree.carseats)
cv.Carseats
plot(cv.Carseats$size,cv.Carseats$dev,type="b")
# best k is 21 which is original size, so there is no reason to prune
#prune.carseats=prune.tree(tree.carseats,best=<Best K>)
#d) bagging
set.seed(42)
bag.Carseats=randomForest(Sales~.,data=Carseats.train,mtry=10,importance=TRUE)
bag.Carseats
yhat.bag = predict(bag.Carseats, newdata=Carseats.test)
plot(yhat.bag, Carseats.test$Sales)
abline(0,1)
mean((yhat.bag- Carseats.test$Sales)^2)
#e) random forest
set.seed(42)
bag.Carseats=randomForest(Sales~.,data=Carseats.train,mtry=4,importance=TRUE)
bag.Carseats
yhat.bag = predict(bag.Carseats, newdata=Carseats.test)
plot(yhat.bag, Carseats.test$Sales)
abline(0,1)
mean((yhat.bag- Carseats.test$Sales)^2)
#d) bagging
set.seed(42)
bag.Carseats=randomForest(Sales~.,data=Carseats.train,mtry=10,importance=TRUE)
bag.Carseats
yhat.bag = predict(bag.Carseats, newdata=Carseats.test)
plot(yhat.bag, Carseats.test$Sales)
abline(0,1)
mean((yhat.bag- Carseats.test$Sales)^2)
yhat.bag = predict(bag.Carseats, newdata=Carseats.train)
plot(yhat.bag, Carseats.train$Sales)
abline(0,1)
mean((yhat.bag- Carseats.train$Sales)^2)
yhat.bag = predict(bag.Carseats, newdata=Carseats.test)
plot(yhat.bag, Carseats.test$Sales)
abline(0,1)
mean((yhat.bag- Carseats.test$Sales)^2)
set.seed(42)
bag.Carseats=randomForest(Sales~.,data=Carseats.train,mtry=3,importance=TRUE)
bag.Carseats
#train
yhat.bag = predict(bag.Carseats, newdata=Carseats.train)
plot(yhat.bag, Carseats.train$Sales)
abline(0,1)
mean((yhat.bag- Carseats.train$Sales)^2)
#test
yhat.bag = predict(bag.Carseats, newdata=Carseats.test)
plot(yhat.bag, Carseats.test$Sales)
abline(0,1)
mean((yhat.bag- Carseats.test$Sales)^2)
set.seed(42)
bag.Carseats=randomForest(Sales~.,data=Carseats.train,mtry=4,importance=TRUE)
bag.Carseats
#train
yhat.bag = predict(bag.Carseats, newdata=Carseats.train)
plot(yhat.bag, Carseats.train$Sales)
abline(0,1)
mean((yhat.bag- Carseats.train$Sales)^2)
#test
yhat.bag = predict(bag.Carseats, newdata=Carseats.test)
plot(yhat.bag, Carseats.test$Sales)
abline(0,1)
mean((yhat.bag- Carseats.test$Sales)^2)
set.seed(42)
bag.Carseats=randomForest(Sales~.,data=Carseats.train,mtry=5,importance=TRUE)
bag.Carseats
#train
yhat.bag = predict(bag.Carseats, newdata=Carseats.train)
plot(yhat.bag, Carseats.train$Sales)
abline(0,1)
mean((yhat.bag- Carseats.train$Sales)^2)
#test
yhat.bag = predict(bag.Carseats, newdata=Carseats.test)
plot(yhat.bag, Carseats.test$Sales)
abline(0,1)
mean((yhat.bag- Carseats.test$Sales)^2)
set.seed(42)
bag.Carseats=randomForest(Sales~.,data=Carseats.train,mtry=5,importance=TRUE)
bag.Carseats
#train
yhat.bag = predict(bag.Carseats, newdata=Carseats.train)
plot(yhat.bag, Carseats.train$Sales)
abline(0,1)
mean((yhat.bag- Carseats.train$Sales)^2)
#test
yhat.bag = predict(bag.Carseats, newdata=Carseats.test)
plot(yhat.bag, Carseats.test$Sales)
abline(0,1)
mean((yhat.bag- Carseats.test$Sales)^2)
plot(bag.Carseats)
###########################################################################
# How to tune the rf parameters jointly
###########################################################################
# create predefined folds:
library(caret)
library(mlbench)
train_data <- Boston[train, ]
test_data <- Boston[-train, ]
set.seed(1234)
cv_folds <- createFolds(train_data$medv, k = 5, returnTrain = TRUE)
ctrl <- trainControl(method = "cv",
number = 5,
search = 'grid',
savePredictions = "final",
index = cv_folds
)
##########    rf    ##################
# create tune control:
tuneGrid <- expand.grid(.mtry = c(3,5,7)
)
# define other parameters to tune. I will use just a few combinations to limit the train time of the example:
ntrees <- c(50,10) #it depends on the number of train observation
nodesize <- c(1, 5, 10)
params <- expand.grid(ntrees = ntrees,
nodesize = nodesize)
# train:
store_maxnode <- vector("list", nrow(params))
start.time <- Sys.time()
for(i in 1:nrow(params)){
nodesize <- params[i,2]
ntree <- params[i,1]
set.seed(65)
rf_model <- train(medv~.,
data = train_data,
method = "rf",
importance=TRUE,
tuneGrid = tuneGrid,
trControl = ctrl,
ntree = ntree,
nodesize = nodesize)
store_maxnode[[i]] <- rf_model
}
end.time <- Sys.time()
time.taken <- round(end.time - start.time,2)
time.taken
# In order to avoid generic model names - model1, model2 ... we can name the resulting list with the corresponding parameters:
# names(store_maxnode) <- paste("ntrees:", params$ntrees,
#                               "nodesize:", params$nodesize)
names(store_maxnode) <- paste("ntrees:", params$ntrees, "nodesize:", params$nodesize)
#results
store_maxnode$`ntrees: 50 nodesize: 1`$results
trellis.par.set(caretTheme())
plot(store_maxnode$`ntrees: 50 nodesize: 1`)
# combine results:
results_mtry <- resamples(store_maxnode)
summary(results_mtry)
results_mtry$timings
dotplot(results_mtry)
# To get the best mtry for each model:
lapply(store_maxnode, function(x) x$best)
# or alternatively to get the best average performance for each model
lapply(store_maxnode, function(x) x$results[x$results$RMSE == min(x$results$RMSE),])
###########################################################################
# 5.8 Exploring and Comparing Resampling Distributions
###########################################################################
##############################
# 5.8.1 Within-Model
##############################
trellis.par.set(caretTheme())
densityplot(store_maxnode$`ntrees: 50 nodesize: 1`, pch = "|")
histogram(store_maxnode$`ntrees: 50 nodesize: 1`)
##############################
# 5.8.2 Between-Models
##############################
resamps <- resamples(list(model1 = store_maxnode$`ntrees: 50 nodesize: 1`,
model2 = store_maxnode$`ntrees: 50 nodesize: 5`,
model3 = store_maxnode$`ntrees: 50 nodesize: 10`))
resamps
summary(resamps)
theme1 <- trellis.par.get()
theme1$plot.symbol$col = rgb(.2, .2, .2, .4)
theme1$plot.symbol$pch = 16
theme1$plot.line$col = rgb(1, 0, 0, .7)
theme1$plot.line$lwd <- 2
trellis.par.set(theme1)
bwplot(resamps, layout = c(3, 1))
trellis.par.set(caretTheme())
dotplot(resamps, metric = "RMSE")
1:11
rf_model
plot(rf_model$results$mtry, rf_model$results$RMSE)
# To get the best mtry for each model:
lapply(store_maxnode, function(x) x$best)
# or alternatively to get the best average performance for each model
lapply(store_maxnode, function(x) x$results[x$results$RMSE == min(x$results$RMSE),])
